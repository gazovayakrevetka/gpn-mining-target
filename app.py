import io
from pathlib import Path
from typing import Tuple

import joblib
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import streamlit as st


BASE_DIR = Path(__file__).resolve().parent

NUMERIC_FEATURES_BASE = [
    "X",
    "Y",
    "Z",
    "Ore_Grade (%)",
    "Tonnage",
    "Ore_Value (USD/tonne)",
    "Mining_Cost (USD)",
    "Processing_Cost (USD)",
    "Waste_Flag",
]
DERIVED_FEATURES = ["Total_Cost_per_tonne"]
CATEGORICAL_FEATURES = ["Rock_Type"]
TARGET_COLUMN = "Target"


@st.cache_resource
def load_artifacts():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å, –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫, —Å–∫–µ–π–ª–µ—Ä –∏ –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ –ø–∞–ø–∫–∏ GPN2."""
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–æ–≤ –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π
    required_files = {
        "best_model.pkl": BASE_DIR / "best_model.pkl",
        "encoder.pkl": BASE_DIR / "encoder.pkl",
        "scaler.pkl": BASE_DIR / "scaler.pkl",
    }
    
    missing_files = []
    for name, path in required_files.items():
        if not path.exists():
            missing_files.append(name)
    
    if missing_files:
        raise FileNotFoundError(
            f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Ñ–∞–π–ª—ã –º–æ–¥–µ–ª–∏: {', '.join(missing_files)}. "
            f"–¢–µ–∫—É—â–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {BASE_DIR}. "
            f"–ó–∞–≥—Ä—É–∑–∏—Ç–µ —ç—Ç–∏ —Ñ–∞–π–ª—ã –Ω–∞ GitHub –≤ –∫–æ—Ä–µ–Ω—å —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è."
        )
    
    best_model = joblib.load(BASE_DIR / "best_model.pkl")
    encoder = joblib.load(BASE_DIR / "encoder.pkl")
    scaler = joblib.load(BASE_DIR / "scaler.pkl")

    metrics_text = ""
    metrics_json = BASE_DIR / "model_performance.json"
    metrics_txt = BASE_DIR / "model_performance.txt"
    if metrics_json.exists():
        metrics_text = metrics_json.read_text(encoding="utf-8")
    elif metrics_txt.exists():
        metrics_text = metrics_txt.read_text(encoding="utf-8")

    metadata_path = BASE_DIR / "preprocessing_metadata.pkl"
    metadata = joblib.load(metadata_path) if metadata_path.exists() else {}

    return best_model, encoder, scaler, metrics_text, metadata


def preprocess_dataframe(
    df: pd.DataFrame, encoder, scaler
) -> Tuple[np.ndarray, pd.DataFrame]:
    """
    –ü—Ä–∏–≤–æ–¥–∏—Ç DataFrame –∫ —Ñ–æ—Ä–º–∞—Ç—É, –æ–∂–∏–¥–∞–µ–º–æ–º—É –º–æ–¥–µ–ª—å—é:
    - –ø—Ä–∏–≤–æ–¥–∏–º —Ç–∏–ø—ã,
    - –∑–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏,
    - —Å–æ–∑–¥–∞—ë–º Total_Cost_per_tonne,
    - –ø—Ä–∏–º–µ–Ω—è–µ–º scaler –∏ encoder.
    """
    df = df.copy()

    if "Waste_Flag" in df.columns and df["Waste_Flag"].dtype == bool:
        df["Waste_Flag"] = df["Waste_Flag"].astype(int)

    for col in NUMERIC_FEATURES_BASE:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors="coerce")
            df[col] = df[col].fillna(df[col].median())
        else:
            df[col] = 0.0

    for col in CATEGORICAL_FEATURES:
        if col in df.columns:
            df[col] = df[col].fillna("Unknown").astype(str)
        else:
            df[col] = "Unknown"

    df["Total_Cost_per_tonne"] = df["Mining_Cost (USD)"] + df["Processing_Cost (USD)"]

    numeric_features = NUMERIC_FEATURES_BASE + DERIVED_FEATURES
    cat_features = CATEGORICAL_FEATURES

    X_num = scaler.transform(df[numeric_features])
    X_cat = encoder.transform(df[cat_features]) if cat_features else np.empty(
        (X_num.shape[0], 0)
    )

    X = np.hstack([X_num, X_cat])
    return X, df


def single_block_input() -> pd.DataFrame:
    """–§–æ—Ä–º–∞ –≤–≤–æ–¥–∞ –æ–¥–Ω–æ–≥–æ –±–ª–æ–∫–∞ –≤ —Å–∞–π–¥–±–∞—Ä–µ."""
    st.sidebar.subheader("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –±–ª–æ–∫–∞")

    x = st.sidebar.number_input("X", value=0.0)
    y = st.sidebar.number_input("Y", value=0.0)
    z = st.sidebar.number_input("Z", value=0.0)

    ore_grade = st.sidebar.number_input("Ore_Grade (%)", min_value=0.0, value=1.0)
    tonnage = st.sidebar.number_input("Tonnage", min_value=0.0, value=1000.0)
    ore_value = st.sidebar.number_input(
        "Ore_Value (USD/tonne)", min_value=0.0, value=50.0
    )
    mining_cost = st.sidebar.number_input(
        "Mining_Cost (USD)", min_value=0.0, value=20.0
    )
    processing_cost = st.sidebar.number_input(
        "Processing_Cost (USD)", min_value=0.0, value=10.0
    )
    waste_flag = st.sidebar.checkbox("Waste_Flag (1 = waste, 0 = ore?)", value=False)

    _, encoder, _, _, _ = load_artifacts()
    if CATEGORICAL_FEATURES and hasattr(encoder, "categories_"):
        rock_categories = list(encoder.categories_[0])
    else:
        rock_categories = ["Type_A", "Type_B", "Type_C"]
    rock_type = st.sidebar.selectbox("Rock_Type", rock_categories)

    data = {
        "X": x,
        "Y": y,
        "Z": z,
        "Ore_Grade (%)": ore_grade,
        "Tonnage": tonnage,
        "Ore_Value (USD/tonne)": ore_value,
        "Mining_Cost (USD)": mining_cost,
        "Processing_Cost (USD)": processing_cost,
        "Waste_Flag": int(waste_flag),
        "Rock_Type": rock_type,
    }
    return pd.DataFrame([data])


def batch_input() -> pd.DataFrame:
    """–ó–∞–≥—Ä—É–∑–∫–∞ CSV/XLSX —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –±–ª–æ–∫–∞–º–∏."""
    st.subheader("–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ —Å –±–ª–æ–∫–∞–º–∏")
    uploaded_file = st.file_uploader(
        "–ó–∞–≥—Ä—É–∑–∏—Ç–µ CSV –∏–ª–∏ Excel (XLSX) —Ñ–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏ –±–ª–æ–∫–æ–≤", type=["csv", "xlsx"]
    )
    if not uploaded_file:
        return pd.DataFrame()

    if uploaded_file.name.lower().endswith(".csv"):
        df = pd.read_csv(uploaded_file)
    else:
        df = pd.read_excel(uploaded_file)

    st.write("–ü–µ—Ä–≤—ã–µ —Å—Ç—Ä–æ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞:")
    st.dataframe(df.head())
    return df


def plot_prediction_vs_actual(df: pd.DataFrame, y_pred: np.ndarray) -> None:
    """–°—Ç—Ä–æ–∏—Ç scatter plot –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –ø—Ä–æ—Ç–∏–≤ —Ñ–∞–∫—Ç–∞ –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ Target."""
    if TARGET_COLUMN not in df.columns:
        st.info("–í –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–º —Ñ–∞–π–ª–µ –Ω–µ—Ç —Å—Ç–æ–ª–±—Ü–∞ Target ‚Äì –≥—Ä–∞—Ñ–∏–∫ –Ω–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω.")
        return

    valid_mask = df[TARGET_COLUMN].notna()
    if valid_mask.sum() == 0:
        st.info("–í–æ –≤—Å–µ—Ö —Å—Ç—Ä–æ–∫–∞—Ö Target = NaN ‚Äì –≥—Ä–∞—Ñ–∏–∫ –Ω–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω.")
        return

    y_true = df.loc[valid_mask, TARGET_COLUMN]
    y_pred_valid = y_pred[valid_mask.values]

    plt.figure(figsize=(6, 6))
    plt.scatter(y_true, y_pred_valid, alpha=0.6)
    max_val = max(y_true.max(), y_pred_valid.max())
    min_val = min(y_true.min(), y_pred_valid.min())
    plt.plot([min_val, max_val], [min_val, max_val], "r--", label="y = x")
    plt.xlabel("–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–π Target")
    plt.ylabel("–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π Target")
    plt.title("–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ vs –§–∞–∫—Ç")
    plt.legend()
    st.pyplot(plt.gcf())
    plt.close()


def add_css() -> None:
    """–ù–µ–±–æ–ª—å—à–æ–π CSS –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤–Ω–µ—à–Ω–µ–≥–æ –≤–∏–¥–∞."""
    st.markdown(
        """
        <style>
        .main {
            background-color: #f7f7f7;
        }
        .block-container {
            padding-top: 1rem;
            padding-bottom: 1rem;
        }
        h1, h2, h3 {
            color: #003366;
        }
        </style>
        """,
        unsafe_allow_html=True,
    )


def main():
    st.set_page_config(
        page_title="–ü—Ä–æ–≥–Ω–æ–∑ Target –¥–ª—è –≥–æ—Ä–Ω—ã—Ö –±–ª–æ–∫–æ–≤",
        layout="wide",
    )
    add_css()
    st.title("–ü—Ä–æ–≥–Ω–æ–∑ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –≤ –¥–∞–Ω–Ω—ã—Ö –≥–æ—Ä–Ω–æ–π –¥–æ–±—ã—á–∏")

    st.markdown(
        """
        –≠—Ç–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π **Target**
        –ø–æ –¥–∞–Ω–Ω—ã–º –±–ª–æ–∫–∞ (–∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã, —Ç–∏–ø –ø–æ—Ä–æ–¥—ã, —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ —Ä—É–¥—ã, —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –∏ –ø—Ä.).
        """
    )

    # –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è (–º–æ–∂–Ω–æ —É–±—Ä–∞—Ç—å –ø–æ—Å–ª–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è)
    with st.expander("üîç –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è", expanded=False):
        import os
        files_in_dir = sorted([f for f in os.listdir(BASE_DIR) if os.path.isfile(BASE_DIR / f)])
        st.write(f"**–¢–µ–∫—É—â–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è:** `{BASE_DIR}`")
        st.write(f"**–§–∞–π–ª—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏:** {len(files_in_dir)}")
        st.write("**–°–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤:**")
        for f in files_in_dir[:20]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 20
            st.write(f"- `{f}`")
        if len(files_in_dir) > 20:
            st.write(f"... –∏ –µ—â—ë {len(files_in_dir) - 20} —Ñ–∞–π–ª–æ–≤")

    try:
        model, encoder, scaler, metrics_text, metadata = load_artifacts()
    except FileNotFoundError as e:
        st.error(
            f"‚ùå **–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {e}**\n\n"
            "**–ß—Ç–æ –Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å:**\n"
            "1. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤—ã –∑–∞–ø—É—Å—Ç–∏–ª–∏ –ª–æ–∫–∞–ª—å–Ω–æ `2_preprocessing.py` –∏ `3_model_training.py`\n"
            "2. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –Ω–∞ GitHub —Å–ª–µ–¥—É—é—â–∏–µ —Ñ–∞–π–ª—ã –∏–∑ –ø–∞–ø–∫–∏ GPN2:\n"
            "   - `best_model.pkl`\n"
            "   - `encoder.pkl`\n"
            "   - `scaler.pkl`\n"
            "   - `model_performance.txt` (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n"
            "   - `preprocessing_metadata.pkl` (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n"
            "3. –ü–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –Ω–∞ Streamlit Cloud"
        )
        st.stop()
    except Exception as e:
        st.error(
            f"‚ùå **–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏:**\n\n`{e}`\n\n"
            "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏ –≤ Streamlit Cloud –¥–ª—è –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–µ–π."
        )
        st.stop()

    col_left, col_right = st.columns([1, 2])

    with col_left:
        with st.expander("–û –º–æ–¥–µ–ª–∏ –∏ –ø–∞–π–ø–ª–∞–π–Ω–µ", expanded=True):
            st.write(
                """
                1. **1_eda.py** ‚Äì —Ä–∞–∑–≤–µ–¥–æ—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –∏ –≥—Ä–∞—Ñ–∏–∫–∏.
                2. **2_preprocessing.py** ‚Äì –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞, –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ.
                3. **3_model_training.py** ‚Äì –æ–±—É—á–µ–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π –∏ –≤—ã–±–æ—Ä –ª—É—á—à–µ–π.
                4. **app.py** ‚Äì —ç—Ç–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏.
                """
            )

        with st.expander("–ú–µ—Ç—Ä–∏–∫–∏ –º–æ–¥–µ–ª–∏", expanded=True):
            if metrics_text:
                st.text(metrics_text)
            else:
                st.info("–§–∞–π–ª model_performance.txt –Ω–µ –Ω–∞–π–¥–µ–Ω.")

        with st.expander("–ü–æ—è—Å–Ω–µ–Ω–∏—è –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º"):
            st.markdown(
                """
                - **X, Y, Z** ‚Äì –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –±–ª–æ–∫–∞ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ.
                - **Rock_Type** ‚Äì —Ç–∏–ø –ø–æ—Ä–æ–¥—ã (–∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫).
                - **Ore_Grade (%)** ‚Äì —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –ø–æ–ª–µ–∑–Ω–æ–≥–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ –≤ —Ä—É–¥–µ.
                - **Tonnage** ‚Äì —Ç–æ–Ω–Ω–∞–∂ –±–ª–æ–∫–∞.
                - **Ore_Value (USD/tonne)** ‚Äì —Å—Ç–æ–∏–º–æ—Å—Ç—å —Ä—É–¥—ã –∑–∞ —Ç–æ–Ω–Ω—É.
                - **Mining_Cost (USD)** ‚Äì –∑–∞—Ç—Ä–∞—Ç—ã –Ω–∞ –¥–æ–±—ã—á—É –∑–∞ —Ç–æ–Ω–Ω—É.
                - **Processing_Cost (USD)** ‚Äì –∑–∞—Ç—Ä–∞—Ç—ã –Ω–∞ –ø–µ—Ä–µ—Ä–∞–±–æ—Ç–∫—É –∑–∞ —Ç–æ–Ω–Ω—É.
                - **Waste_Flag** ‚Äì —Ñ–ª–∞–≥, —É–∫–∞–∑—ã–≤–∞—é—â–∏–π –Ω–∞ –ø—É—Å—Ç—É—é –ø–æ—Ä–æ–¥—É.
                - **Total_Cost_per_tonne** ‚Äì —Å—É–º–º–∞—Ä–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã –Ω–∞ —Ç–æ–Ω–Ω—É (—Å–æ–∑–¥–∞–Ω–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫).
                """
            )

    with col_right:
        mode = st.radio(
            "–†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã",
            ["–û–¥–∏–Ω –±–ª–æ–∫", "–§–∞–π–ª —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –±–ª–æ–∫–∞–º–∏"],
            horizontal=True,
        )

        if mode == "–û–¥–∏–Ω –±–ª–æ–∫":
            df_input = single_block_input()
            if st.button("–ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å", type="primary"):
                X, df_proc = preprocess_dataframe(df_input, encoder, scaler)
                y_pred = model.predict(X)[0]
                st.success(f"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ Target: **{y_pred:.4f}**")
                st.write("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:")
                st.dataframe(df_proc)
        else:
            df_batch = batch_input()
            if not df_batch.empty and st.button("–ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –¥–ª—è —Ñ–∞–π–ª–∞", type="primary"):
                X, df_proc = preprocess_dataframe(df_batch, encoder, scaler)
                y_pred = model.predict(X)
                df_result = df_proc.copy()
                df_result["Predicted_Target"] = y_pred

                st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è")
                st.dataframe(df_result.head(50))

                csv_buf = io.StringIO()
                df_result.to_csv(csv_buf, index=False)
                st.download_button(
                    label="–°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ CSV",
                    data=csv_buf.getvalue().encode("utf-8-sig"),
                    file_name="predictions.csv",
                    mime="text/csv",
                )

                plot_prediction_vs_actual(df_batch, y_pred)


if __name__ == "__main__":
    main()


